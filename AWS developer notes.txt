1. EC2 instance storage:(72)

EBS: 
elastic block store volume is a network drive attaches to instances.
persist data even after their termination and can be mounted to one instace (ccp) but at developer level multi attach feature is there.
locked to one az if want to move use snapshot
to use we need to provide the capacity like GBs and IOPS

by default if rooted EBS is deleted on termination where other attached EBS vols are not deleted.

EBS Snapshots:
Backup for EBS , is incremental backup , can copy across az or region 
features: 
1. EBS Snapshot Archive: archive tier(75% cheaper) takes 24-72 hrs to restore
2. Recycle Bin: recycle bin for accidental deletion and retention is 1day-1yr
3. Fast Snapshot Restore(FSR): force full initialization of snapshot for no latency on first use(expensive)

AMI(Amazon Machine Image):
customization of ec2 instance, can add our software, config., OS, monitoring.,etc. Faster boot/config. time because all software is pre-packaged.
built for specific region( copied across regions)
launch ec2 intances from public AMI, our own AMI , our AWS Marketplace AMI

process: start ec2 and customize, stop instance(for data integrity), Build an AMI(creates snapshots), launches instances from other AMIs

EC2 instance store:
high-performance hardware disk, better I/O performance, if instance is stopped they will loss their storage, use as buffer/cache/scratch data/temporary content, risk of data loss if hardware fails, backups and replication are our responsiblility.

EBS Volume types(6 types):
--------------------------------------
GP ( general purpose SSD volume balances price and perf.) 
-cost effective, low latency 
-1 GiB - 16 TiB
-system boot vols, virtual desktops, dev and test envs

1. gp2(SSD): 
small gp2 vol can burst IOPS to 3,000,
size of vol and IOPS are linked, max is 16,000
3 IOPS per GB, means 5,334 GB we are at max IOPS

2. gp3(SSD): 
baseline 3,000 IOPS and throughput 125 MiB/s
can increase 16,000 IOPS and throughput upto 1000 MiB/s independently
---------------------------------
Provisioned IOPS(PIOPS)(highest perf. SSD vol for mission critical low latency or high throughput workloads)
-apps that need more than 16000 IOPS
- great for database workloads (sensitive to storage perf and consistency)

3. io 1:(4GiB - 16 TiB)
max PIOPS 64,000 for nitro instances and 32,000 for other
increase PIOPS independently,more durability and more IOPS per GiB

4. io 2:( 4 GiB - 64 TiB)
sub-millisecond latency 
max PIOPS 256,000 with IOPS:GiB is 1,000:1
-----------------------------------------------------
(Low cost HDD )
can't be used for boot vol
125 GiB - 16 TiB

5. st 1:(frequently accessed) throughput optimized 
Big data, data warehouses, log processing 
max throughput 500 MiB/s, max IOPS 500

6. sc 1: (less freq. accessed) Cold HDD
lowest cost
max throughput 250 MiB/s, max IOPS 250
--------------------------------------------
EBS Multi-Attach :
for only provisioned iops family io1,io2
attach same EBS vol to multiple ec2 instances in same AZ
each instance has full read& write permissions to high-perf. vol
use case: achieve high application availability in clustered linux apps(eg: teradata)
	applications must manage concurrent write ops.
upto 16 instances only at a time	
file system must be cluster-aware (not XFS, EXT4)


Amazon EFS- Elastic File System
Managed NFS (network file system) can be mounted to many eC2
EFS works with instances in multi-AZ
highly available, scalable , expensive(3x gp2), pay per use
use cases: content management , web serving, data sharing, Wordpress
uses interanl NFSv4 protocol
uses security grp to control access to EFS
compatible with linux based AMIs(not windows)
POSIX file system(linux) that has std file API

EFS scale: 1000s of concurrent NFS client, 10 GB+/s throughput
grow petabyte scale nfs automatically 

performance Mode(set at EFS creation time)
general purpose(default): latency sensitive use cases (webservers, CMS etc)
max I/O-higher latency,throughput,highly parallel(bigdata,media processing)

throughput Mode:
Bursting: 1 TB= 50 MiB/s + burst of 100 MiB/s
Provisioned: set throughput regardless storage size(eg:1 GiB/s for 1 Tb storage)
Elastic:  automatically scales up and down based on workloads
upto 3 GiB/s for reads and 1 GiB/s for write 
used for unpredictable workloads

EFS- storage classes
storage Tiers ( lifecycle management feature- move file after N days)
std : for freq. access
infreq access (EFS-IA): cost to retrieve files, lower price to store, enable EFS-IA with lifecycle policy

availability and durability:
std: multi az, great for prod
one zone : one Az, grear for dev, back enabled by default, compatible with IA(EFS One Zone-IA) 
90% in cost savings


EBS vs EFS 

EBS: one instance at time( except io1& io2)
locked at az level, gp2: increases IO if disk size increases, gp3: IO can increased independently
to migrate across az: need snapshots and restore at respective az , don't backup while application is handling lot of traffic.

EFS: mounted to 100s of instances across az, EFS share website files (wordpress), only for linux instances, costly than EBS, can leverage EFS-IA for cost savingsf
==============================================================================================
2. AWS Fundamentals: ELB+ASG(95)

scalability: by adapting the application can handle greater loads
a. vertical scalability(increase size)
for non distributed systems like databases like RDS , ElastiCache can scale vertically 

b. horizontal scalability(increase number)(elasticity)
for distributed systems, common web apps/modern apps

High Availability
having application at least 2 Azs, to survive a data center loss, can be passive for RDS Multi-Az and active for horizontal scaling

Load balancing:
servers that forward traffic to multiple servers downstream

ELB(Elastic Load Balancer):
managed load balancer, aws takes care of upgrades, maintenance, high availability and few config. knobs
if we have our own LB but it will cost less but will be alot more effort on our end
it's integrated with many aws services like ec2, ASG, ECS, ACM,CloudWatch, Route 53, WAF,AWS Global Accelerator

Health Checks:(crucial for Load balancers)
enable LB to know if instances it forwards traffic to are available to reply to request and it's done on a port and route 

Types of load balancers
Classic LB: old generation-2009: HTTP,HTTPS,TCP,SSL(Secure TCP)
Application LB: new -2016: HTTP,HTTPS,WebSocket
Network LB:new-2017: TCP, TLS(secure TCP), UDP
Gateway LB:2020:Operates at layer 3 (network)- IP protocol

Application Load Balancer (ALB) layer 7(HTTP)
-load balancing to multiple HTTP apps across machines(target grps)
-load balancing to multiple apps on the same machine (eg:containers)
-support for HTTP/2 and WebSocket
-support redirects ( from HTTP to HTTPS for eg)

Routing tables to diff target grps:
routing based on : 
	-path in url(example.com/users & example.com/posts)
	-hostname in url (one.example.com & other.example.com)
	-Query String,Headers (example.com/users?id=123&order=false)
ALB are great fit for microservices & container-based apps (docker & ECS)
has port mapping feature to redirect to a dynamic port in ECS

ALB target grps:
ec2 instances ( managed by ASG) -HTTP
ECS tasks( managed by ECS itself)- HTTP
lambda fns - HTTP req is translated into JSON event 
IP Address-must be private IP's 
ALB can route to multi target grps
Health checks are at target grp level

while connecting through ALB from client to instance
client Ip can't be known by instance instead its knows ip which is private ip fo ALB
if want know the ip of client it will look into extra headers in HTTP req which are port and proto (x-forward-for)

Network Load Balancer (NLB) (layer 4) allows
-forward TCP & UDP traffic to instances
-Handle millions of req per sec
-less latency 100ms(400ms for ALB)
one static IP per Az and supports assigning Elastic IP 
used for extreme performance, TCP or UDP traffic

NLB targets grps: ec2 instances, IP addressess(private), ALB
Health checks support TCP, HTTP & HTTPS

Gateway Load Balancer(GLB)(layer 3)Network layer
deploy,scale & manage a fleet of 3rd party network virtual appliances in aws
eg: firewalls, intrusion detection & prevention sys,deep packet inspection sys,payload manipulation
combines: transparent network gateway: single entry/exti for all traffic
	  Load balancer: distributes traffic to our virtual appliances
- uses the GENEVE protocol on port 6081

Sticky Sessions: 
stickiness so that same client is always redirects to the same instance behind a load balancer 
works for CLB,ALB,NLB
cookie is used for expiration date of control
to dont't lose the session data
Application-based cookies:
custom cookie : generated by us (don't us names AWSALB,AWSALBAPP,AWSALBTG)
application : generated by LB
Duration-based cookie:generated by cookie for a period of time


Cross-Zone load balancing
ALB:enabled by default(disabled at target grp) and no charge
NLB&GLB: disabled by default, charges for inter AZ data if enabled
CLB:disabled by default, no charges if enabled

SSL/TLS (Secure Sockets Layer/Transport Layer Security(new))
SSL certficate allows traffic between clients and load balancer to be encrypted in transit.
-is issued by Certificate Authorities(CA)
-Comodo,Symantec,GoDaddy,GlobalSign,Digicert etc..
-load balancer uses X.509 certificate 
-manage certificates using ACM(aws certificate manager)
-can create own certificates also
-HTTPS listener: 
specify default certificate 
can add optional list of certificates to support multiple domains
-clients use SNI(Server Name Indication)to specify the hostname they reach

Sever Name Indication (SNI)
solves the problem of loading multiple SSL certificates onto the webserver (to serve multiple websites)
newer protocol and requires the client to indicate the hostname of the target server in the initial SSL handshake 
server will find correct cert or return the default one 
-only works for ALB & NLB 

Connection draining(CLB)/Deregrestration Delay(ALB&NLB):
time to complete "in flight requests" while the instance is de-registering or unhealthy
-stops sending new reqs to that instance 
between 1 to 3600 seconds (def : 300 seonds)
set to low value if req is short

Auto Scaling Group(ASG)
-scale out(add instances) when increased load and scale in (remove instances) when decreased load.
-will have maxi and mini number of instances running 
-recreate the instance incase one is unhealthy
-free
-attribute of ASG , a launch template which has AMI+Instance type, ec2 user data, EBS volumes, Security grps, SSH key pairs, IAM roles for instances, Network+Subnet information and Load Balancer information
- can also be integrated with CloudWatch by triggering the alarms to scale respectively.

ASG - dynamic scaling policies
target tracking scaling :most simple and easy 
simple/step scaling:add and remove based on trigger of cloudwatch alarms 
scheduled actions: anticipate a scaling on known usage patterns
predictive scaling: continously forecast load and schedule scaling ahead
good metrics to scale on: CPUUtilization , RequestCountPerTarget, Average Network In/out

ASG scaling cooldowns
after scaling activity we are in a cooldown period of 300 sec
during this ASG will not launch or terminate additional instances

ASG Instance Refresh
to update launch template and then recreate all ec2 instances 
we can use native feature of instance refresh and setting of minimum healthy percentage like 60%

-------------------------------------------------------------------------------------------------------------------------------------------------------
Amazon RDS (slide: 138)

relational database (managed database) uses SQL query language 
has postgres, mysql, MariaDb, Oracle, Microsoft SQL server, Aurora
-automated provisioning, OS patching
-continuonus backups and restore to specific timestamp
-Monitoring dashboards
-Read Replicas for improved read perform
-Mutli az setup for disaster recovery
-Maintenance windows for upgrades
-Scaling capability
-Storage backed by EBS(gp2 or io1)
-we can't SSH into instances cause RDS is managed service

RDS -storage auto scaling
-when RDS detects running out of free database storage, it scales automatically
-can set maximum storage threshold 
-automatically modify storage if: 
	-free storage is less than 10% of allocated storage
	-low-storage lasts at least 5 min 
	-6 hrs have passed since last modification
- uses for unpredictable workloads

RDS read replicas:
upto 15
within az, cross az, cross region 
replication is ASYNC so reads eventually consistent
replicas can be promoted to their own DB
read replicas ae used for SELECT only kind of statements (not INSERT, DELETE, UPDATE)
in aws there's a network cost when data goes from one az to other 
- for RDS read replicas within same region, we don't pay any

RDS Multi Az(Disaster Recovery)
SYNC replication
one DNS name -automatic app failover to standby
increase availability
not used for scaling 
- read replicas can also be setup as Multi Az for disaster recovery 
- if single az to multi az: zero downtime operation, just click on modify and following happens interally:
	-a snapshot is taken
	-a new DB is restored from the snapshot in new AZ
	-Synchronization is established between the two databases

Amazon Aurora (slide 147)

proprietary technology from AWS (not opensourced)
Postgres (3x) and MySQL(5x) are supported and have improve performance of RDS
automatically grows in increaments of 10GB to 128TB
have upto 15 replicas and replication processs is faster than MySQL

Aurora high availability and read scaling
6 copies of data across 3 az:
4 copies of 6 for writes , 3copies of 6 for reads 
self healing .storage is striped across 100s of vols
one master writes+ 15 read replics(upto)
automated failover for master in less than 30 sec
support cross region replication

will have two endpoints 
Writer endpoint to master 
Reader endpoint( with load balancer at connection level ) to read replicas with auto scaling

RDS & Aurora Security
at rest:
databases master & replicas encryption using KMS (defined at launch time)
if master is not encrypted, the read replicas cannot be encrypted
to encrypt an un-encrypted database, go through snapshot & restore as encrypted

in flight:
TLS- ready by default, use the AWS TLS root certifcates client side
IAM authentication: by iam roles 
security grps: control network access to RDS (block or allow ip,security grps, ports)
No SSH except for RDS Custom 
Audit logs can enabled and sent to CloudWatch Logs for longer retention

Amazon RDS Proxy:
full managed database proxy for RDS
allows apps to pool and share DB connections established with database
improves database efficiency by reducing stress on database resources and minimize open connections ( and timeouts)
serverless(integrity with lambdas),autoscaling highly avialable(multi az)
reduced RDS & Aurora failover time (66%) using proxy
enforce IAM authentication for DB, and securely store credentials in Secrets Manager
RDS proxy is never publicy accessible ( only by VPC)
 
Amazon ElasticCache 
managed Redis or Memcached
in-memory databases with high performance, low latency
reduces load off of databases for read intensive workloads, makes app stateless
REDIS : multi az , read replicas, data durability using AOF persistence,Backup and restore, supports sets and sorted sets

MEMCACHED : multi-node of partitioning of data(sharding),no high avail, non persistent, no backup and restore, multi-thread architecture

caching strategies
Lazy loading/ cache-side/lazy population:
pros: only req data is cached and node failure are not fatal 
cons: 
	cache miss penalty that results 3 round trips, noticeable delay for that req, 
	stale data: data can be updated in the database and outdated in the cache

Write Through-add or update cache when database is updated:
pros: data in cache is never stale, reads are quick  and write penalty vs read penalty(each write requires 2 calls)
cons: missing data until it is added/updated in the DB. Mitigatin is to implement Lazy Loading strategy as well

Cache Evictions and Time-to-live(TTL)
cache eviction can occur in three ways:
you delete the item explictly in the cache
item is evicted because the memory is full and it's not recently used(LRU)
you set an item time-to-live(TTL)
-TTL are good for leaderboards, comments,activity streams
-TTL can range from few seconds to hours or days
 
Amazon MemoryDB for Redis:
Redis-compatible, durable.in-memory database service
ultra fast performance with 160 millions of req/second
durable in-memory data storage with multi az transactional log
scale seamlessly from 10s GBs to 100s TBs of storage 
use cases: web and mobile apps, online gaming, media streaming

========================================================================================================

Amazon Route 53:(165)
highly available , scalable, fully managed and authoritative(means customer (you) can update the DNS records) DNS
-domain Registrar
checks health of resources and only service with 100% SLA  

Records:
Each record contains:
Domain/subdomain Name: eg: example.com
Record type: eg: A or AAAA
value: eg: 12.34.56.67
Routing Policy : how Route53 responds to queries
TTL: amount of time the record cached at DNs resolvers
- it supports follwing DNS record types:
must know : A, AAAA, CNAME, NS
other : CAA, DS , MX, NAPTR, PTR, SOA, TXT, SPF, SRV 

A: maps a hostname to IPv4
AAAA: maps to IPv6
CNAME: maps a hostname to another hostname (but the  target domain name must have A or AAAA record, can't create a CNAME record for the top node of DNS namespace, eg: we can't create example.com but can www.example.com)
NS : Name servers for the Hosted Zone

Hosted Zones: container for records that define how to route tarffic to domain and it subdomains
public hosted Zones:route traffic on the Internet type of records are there
private hosted Zones: route traffic within one or more VPCs(private domain names) type of records are there 
- charge is $0.50 month per hosted zone
and for a domain name it will cost like 12$ per year

Records TTL(Time To Live)
High TTL: eg: 24hrs
less traffic on route 53 
possibly outdated records

Low TTL: eg:  60sec
more traffic on route53(charges increase)
records are outdated for less time
easy to change records

-except for alias records, TTL is mandatory for each DNS record

CNAME vs Alias
Aws resources(load balancer,cloudfront) expose an aws hostname lb 1-1234.us-east-2.elb.amazonaws.com and we want myapp.mydomain.com

cname:
	points a hostname to anyother hostname(app.mydomain.com=>blabla.anything.com
	ONLY FOR NON ROOT DOMAIN(aka something.mydomain.com)
alias:
	points a hostname to an aws resource(app.mydomain.com=>blabla.amazonaws.com)
	works for ROOT DOMAIN and NON ROOT DOMAIN(aka mydomain.com)
	free of charge 
	native health check

Alias Records:
maps hostname to an aws resource
an extension to DNS functionality
automatically recognizes changes in the resource's IP addresses
unlike CNAME it can be used for top node of DNS namespace(zone apex)eg: example.com
alias record is always of type A/AAAA for aws resources(IPv4/IPv6)
alias records targets are : ELB, cloudfront distributions, API gateway,Elastic Beanstalk env,S3 websites, VPC inteface endpoints, Global Accelerator accelerator and route 53 record in same Hosted Zone
-can't use ALIAS record for ec2 DNS name

Simple Routing:
route traffic to a single resource 
can specify multiple values in the same record
if multiple values are returned, a random one is chosen by the client
when Alias enabled, specify only one AWS resource
can't be associated with Health Checks

Weighted Routing:
control the % of the requests that go to each specific resource 
assign each record a relative weight :
	traffic (%)= weight for a specific record /sum of all weights of all records
	weights don't need to sum up 100
DNS records must have the same name and type
can have Health Checks 
usecases: load balancing between regions, testing new app versions
assign a weight of 0 to a record to stop sending traffic to resource 
if all weight are zero then traffic spread equally

Latency Routing:
redirect to resource that has least latency close to us
super helpful when latency for users is a priority
Latency is based on traffic between users and AWS Regions 
Germany users may be directed to US(if that's the lowest latency)
can be associated with Health Checks(has a failover capability)

Health Checks
HTTP health checks are only for public resources(private also via cloudwatch)
Health check=> Automated DNS Failover:
1. Health checks monitor an endpoint (app,server,other service)
2. Health checks that monitor other health checks(calc health checks)
3. Health checks that monitor CloudWatch Alarms(full control) eg: throttles of DynamoDB, alarms on RDS, custom metrics(private)
			
health checks- Monitor an Endpoint
about 15 global health checkers will check the endpoint health
-Healthy/unhealthy Threshold -3(default)
-Supported protocol: HTTP, HTTPS, TCP
- if> 18% of health checkers report the endpoint healthy, route53 considers it healthy.
-ability to choose which locations you want route 53 to use
Health Checks pass only when endpoint responds with 2xx or 3xx status codes
Health Checks can be setup to pass/fail based on text in the first 51200 bytes of the response
configure you router/firewall to allow incoming requests from Route 53 Health Checkers

Route 53- Calculated Health Checks
combine the results of multiple child health checks into single parent health check
we can use OR , AND, NOT on these child health checks
can monitor up 256 child health checks
specify how many of health checks need to pass parent
usage: perform maintenance to our website without causing all health check to fail

Health Checks - private Hosted Zones
route 53 health checkers are outside the VPC
They can't access private endpoints(private VPC or on-premises resource)
we can create CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm itself

Failover Routing:
there will two records one primary and other secondary 
if one failed then by health checks it will connect to secondary 
and if primary gets it's connection then domain will show primary

Geolocation Routing:
routing based on user location
specify location by continent, country or by US state 
should create a DEFAULT record(in case there's no match on location)
usecases: website localization, restrict content distribution, load balancing..

Geoproximity Routing:
route traffic based on geographic location of users and resources
ability to shift more traffic to resources based on defined BIAS
to change the size of geographic region, specify bias value:
	to expand (1 to 99) more traffic to resource
	to shrink (-1 to -99) less traffic 
resouces can be : aws resources(specify AWS region) and Non-AWS resources(specify latitude and longitude)
- we have to use Route 53 Traffic Flow to use this feature 

Route 53 - Traffic Flow:
Simplify the process of creating and maintaining records in large and complex configurations
visual editor to manage complex routing decision trees.
configurations can be saved as Traffic Flow Policy:
	can be applied to different Route 53 Hosted Zones(diff domain names)
	supports versioning

IP-based Routing :
routing is based on client's IP address
we provide a list of CIDRs for our clients and the corresponding endpoints/locations(user-IP-to-endpint mappings)
usecases: Optimize performance, reduce network costs..
eg: route end users from a particular ISP to a specific endpoint 

Multi-Value Routing:
used when routing traffic to multiple resources
Route 53 return multiple values/resources
can be associated with Health Checks(returns only healthy)
up to 8 healthy records are returned for each Multi-Value query
Multi-Value is not substitute for having an ELB

Domain Registar vs DNS service
we buy or register our domain name with Domain Registar typically by paying annual charges(eg: GoDaddy , Amazon Registar inc....)
Domain Registar usually provides DNS service to manage our DNS records
but we can use other DNS service also
eg: purchase domain in GoDaddy and use Route 53 to manage DNS records.

3rd party Registar with Route 53
if we bought domain on 3rd party registar, we can still use Route 53 as DNS Service provider
1. create a Hosted Zone in Route 53
2. Update NS Records on 3rd party website to us Route 53 Name Severs
- every Domain Registar usually comes with some DNS features
======================================================================================================================

VPC & Subnets 

VPC : private network to deploy our resources(regional resource)
Subnets: allows us to partition network inside our VPC(Az resource)
public subnet: accessible from the internet
private subnet: not accessible from the internet 
-to define access to internet and between subnets, we use Route Tables

Internet Gateway & NAT Gateways
1. Internet Gateway: helps our VPC instances connect with the internet
-public subnets have a route to the internet gateway
2. NAT Gateways (AWS-managed) & NAT Instances(self managed) allow instances in our Private Subnets to access internet while remaining private

VPC Flow Logs
capture info about IP traffic going into interfaces:
-VPC Flow logs
-Subnet Flow logs
-Elastic Network Inteface Flow logs
helps to monitor & trouble shoot connectivity issues. eg: subnets to internet, subnet to subnet, internet to subnet
captures network info from AWS managed interfaces too: ELB,ElastiCache,RDS,Aurora,etc
VPC flow logs data can go to S3,CloudWatch logs,& Kinesis Data Firehose

VPC Peering
-connect two VPC, privately using AWS's network
-make them behave as if they were in the same network
-must not have overlapping CIDR(IP address range)
-VPC Peering connection is not transitive(must be established for each VPC that need to communicate with one another)

VPC Endpoints
-Endpoints allow you to connect to AWS Services using private network instead of public(www) network
-this gives enhanced security and lower latency to access AWS services
-VPC Endpoint Gateway: S3 & DynamoDB
-VPC Endpoint Interface: the rest
- only used within VPC

Site to Site VPN & Direct Connect
Site to Site VPN: 
connect an on-premises VPN to AWS
The connection is automatically encrypted 
goes over the public internet
Direct Connect(DX):
Establish a physical connection between on-premises and AWS
The connection is private, secure and fast
goes over a private network
takes least month to establish

3 tier solution architecture:

public subnet(ELB) : private subnet(instances with ASG): data subnet(RDS & ElastiCache)

LAMP Stack on ec2:
Linux: OS foe ec2
Apache: Web Server that run on Linux(ec2)
MySQL: database on RDS
PHP: Application logic (running on ec2)
-can add Redis/Memcached(ElastiCache) to include a caching tech
-to store local app data & software : EBS drive(root)

Wordpress on AWS:
cliet sends images via Route53 to ELB(public subnet) : instances in multi Az(private subnet) : EFS via ENI (data subnet)

========================================================================================================

Amazon S3

infinitely scaling storage service of AWS. (object storage)
usecases: backup&storage, disaster recovery, archive, hybrid cloud storage, application hosting, media hosting, data lakes& big data analytics , software delivery, static website

it allows to store objects(files) in buckets(directories)
buckets must have globally unique name
buckets are defined at regional level 
naming convention: 
no uppercase, no underscore
3-63 chars long
not an IP 
Must start with lowercase letter or number
Must NOT start with prefix xn--
Must NOT end with suffix -s3alias

S3 Objects:
objects have a key(means full pat. eg: s3://my-bucket/my_file.txt,s3://my_folder1/my_folder2/my_file2.txt)
key is composed of prefix+object name - s3://my-bucket/my_folder1/another_folder/my_file.txt
here prefix:my_folder1/another_folder and obj name: my_file.txt
there's no concept of "directories" within buckets
obj values are content of the body: max obj size 5TB and if want to upload 5TB must use "multi-part upload"
metadata(list of text key/value pairs- sytem or user metadata)
Tags(unicode key/value pair - upto 10) - useful for security/ lifecycle
Version ID(if versioning is enabled)

S3 - Security
User-Based: 
IAM policies- which API calls should be allowed for a specific user from IAM
Resource-Based: 
Bucket Policies- bucket wide rules from S3 console-allows cross account
Object Access Control List(Acl)- finer grain (can be disabled)
Bucket Access Control List(Acl)- less common(can be disabled)

-an IAM principal can access S3 obj if user IAM permissions ALLOW "OR" the resource policy ALLOWS it 
"AND" there's no explicit DENY

-Encryption: encrypt objs in Amazon S3 using encryption keys


S3 Bucket policies
JSON based policies:
	Resources: buckets and objs
	Effect: Allow/Deny
	Actions: Set of API to Allow or Deny
	Principal: The account or user to apply the policy to
Use S3 bucket policy to: 
Grant public access to bucket
Force objs to be encrypted at upload
Grant access to another account(CROSS ACCOUNT)


Bucket setting for Block Public Access
to prevent data leaks
if you want ur bucket to be public, leave this on 
can be set at the account level


Amazon S3 - Static Website Hosting
S3 can host static websites and have them accessible on internet
the website url will be(depending on region)
- http://bucket-name.s3-website-aws-region.amazonaws.com
- http://bucket-name.s3-website.aws-region.amazonaws.com
if you get a 403 forbidden error make sure the bucket policy allows public reads

Amazon S3 - Versioning
we can version our files in S3
enabled at bucket level
same key overwrite will change the "version": 1,2,3....
it's best practice to version cause it will protect against unintended deletes and eask roll back to previous version
- any file that is not versioned prior to enabling versioning will have version "null"
- suspending versioning does not delete the previous versions

Amazon S3 - Replication(CRR & SRR)
Must enable Versioning in source and destination buckets
Cross-Region Replication(CRR)
Same-Region Replication(SRR)
Buckets can be in different AWS accounts
Copying is asynchronous
Must give proper IAM permissions to S3
Usecases: 
CRR-compliance,lower latency access, replication across accounts
SRR-log aggregation, live replication between production and test accounts
- After enabling Replication, only new objects are replicated
- optionally, we can replicate existing objs using S3 Batch Replication
replicates existing objs and objs that failed replication
- for DELETE operations
can replicate delete markers from source to target(optional setting)
deletions with version ID are not replicated(to avoid malicious deletes)
- There are no "chaining" of replication
if bucket 1 ha replication into bucket 2, which has replication into bucket 3
then objs created in bucket 1 are not replcated to bucket 3


S3 Storage Classes

S3 Durability and Availability:
Durability: 
High durability (99.99999999999,11 9's) across multiple Az
if you store 10,000,000 objs with S3 we can on avg expect to incur loss of single obj once every 10,000 yrs
and it's same for all storage classes
Availability:
Measures how readily available a service is 
varies depending on storage class
eg: S3 std has 99.99% = not available 53 min a year

1. S3 Standard -general purpose
99.99% availability
used for frequently accessed data
low latency and high throughput
sustain 2 concurrent facility failures
usecases: Big data analytics, mobile & gaming applications, content distribution...

Infrequent Access 
for data that is less frequently accessed, but requires rapid access when needed  and lower cost than S3 std

2. Amazon S3 Standard- Infrequent Access(S3 Standard-IA)
99.99% availability
usecases: Disaster Recovery, backups

3. Amazon S3 One Zone- Infrequent Acces(S3 One Zone-IA)
High durability(99.99999999, 9 9's) in a single Az, data lost when Az is destroyed
99.5% availablilty
usecases: Storing secondary backup copies of on-premise data, or data we can recreate

Amazon S3 Glacier Storage Classes
low cost obj storage meant for archiving/backup
pricing: price for storage + obj retrieval cost

4. Amazon S3 Glacier Instant Retrieval
Millisecond retrieval, great for data accessed once a yr 
minimum storage duration of 90 days

5. Amazon S3 Glacier Flexible Retrieval(formerly Amazon S3 Glacier)
Expedited(1 to 5 min)
Standard ( 3 to 5 hours)
Bulk ( 5 to 12 hours) - freef
minimum storage duration of 90 days 

6. Amazon S3 Glacier Deep Archieve- for long term storage
Standard(12 hours)
Bulk (48 hours)
minimum storage duration of 180 days

7. S3 Intelligent-Tiering
• Small monthly monitoring and auto-tiering fee
• Moves objects automatically between Access Tiers based on usage
• There are no retrieval charges in S3 Intelligent-Tiering
• Frequent Access tier (automatic): default tier
• Infrequent Access tier (automatic): objects not accessed for 30 days
• Archive Instant Access tier (automatic): objects not accessed for 90 days
• Archive Access tier (optional): configurable from 90 days to 700+ days
• Deep Archive Access tier (optional): config. from 180 days to 700+ days
===================================================================================================

AWS CLI, SDK, IAM Roles & Policies

Ec2 Instance Metadata(IMDS)
AWS EC2 Instance Metadata(IMDS) is powerful but one of the least known features to developers
allows AWS EC2 instances to "learn about themselves" without using an IAM Role for that purpose
the URL is http://169.254.169.254/latest/meta-data 
we can retrieve the IAM Role name from the metadata, but we CANNOT retrieve the IAM Policy
Metadata= Info about the EC2 instance
Userdata= launch script of the ec2 instance

IMDSv2 vs IMDSv1
-IMDSv1 is accessing http://169.254.169.254/latest/meta-data directly
-IMDSv2 is more secure and is done in two steps:
1. Get Session Token(limited validity)- using headers & PUT
$ TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
2. Use Session Token in IMDSv2 calls- using headers
$ curl http://169.254.169.254/latest/meta-data/profile -H "X-aws-ec2-metadata-token: $TOKEN" -v 

MFA with CLI
to use MFA with CLI we must create a temporary session
to do so, we must run the STS GetSession Token API call
- aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600


AWS SDK Overview
perform actions on AWS directly from applications code by SDK( software development kit)
official SDKs are JAVA, .NET , NodE.js, PHP, Pytho(named boto3/botocore), Go, Ruby, C++

-we have to use AWS SDK when coding against AWS Services like DynamoDB
-AWS CLI uses the Python SDK(boto3)
-if we don't specify or configure a default region, then us-east-1 will be chosen by default

AWS Limits(Quotas)

API Rate Limits:
DescribeInstances API for EC2 has a limit of 100 calls per seconds
GetObject on S3 has a limit of 5500 GET per second per prefix
For Intermittent Errors: implement Exponential Backoff
For Consistent Errors: request an API throttling limit increase

Servic Quotas(Service Limits)
running On-Demand std Instances: 1152 vCPU
we can request a service limit increase by opening a ticket
we can request a servcie quota increase by using Service Quotas API

Exponential Backoff (any AWS service)
if we get ThrottlingException intermittently, use exponential backoff
Retry mechanism already included in AWS SDK API calls
Must implement ourself if using the AWS API as-is or in specific cases like 
-Must only implement the retries on 5xx server errors and throttling
-don't implement on the 4xx client erros
it goes like 1st try 1sec
2nd 2sec
3rd 4sec
4th 8sec (so that load on application will decrease by this increasing intervals)

AWS CLI Credentials Provider Chain
-the CLI will look for credentials in this order
1. Command line options: --region, --output, --profile
2. Environmental variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN
3. CLI credentials file: awas configure
~/.aws/credentials on Linux / Mac & C:\Users\user\.aws\credentials on Windows
4. CLI configuration file – aws configure
~/.aws/config on Linux / macOS & C:\Users\USERNAME\.aws\config on Windows
5. Container credentials: for ECS tasks
6. Instance profile credentials: for EC2 Instance profiles

AWS SDK Default Credentials Provider Chain
the java SDK(eg) will look for credentials in this order
1. Java system properties: aws.accessKeyId adn aws.secretKey
2. Environmental variables: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
3. The default credentials profiles file: eg at: ~/.aws/credentials, shared by many SDK
4. Amazon ECS container credentials: for ECS containers
5. Instance profile credentials : used on EC2 instances

AWS Credentials Scenario
- an application deployed on an EC2 instance is using env variables with credentials from an IAM user to call the Amazon S3 API
- the IAM  user has S3 FUllAccess permissions
- the application only uses one S3 bucket, so according to best practices:
	-an IAM Role & EC2 Instance Profile was created for the EC2 instance
	-The Role was assigned the minimum permissions to access that one S3 bucket
- the IAM Instance Profile was assigned to the EC2 instance, but it still had access to all S3 buckets because the credentials chain is still giving priorities to the env variables

AWS Credentials Best Practices
- overall, NEVER EVER STORE AWS CREDENTIALS IN THE CODE
- best practice is for credentials to be inherited from the credentials chain
- if using working within AWS, use IAM Roles
=>ec2 instance roles for ec2 instances
=> ECS roles for ECS tasks
=> Lambdha roles for lambda functions
- if working outside of AWS, use env variables / named profiles


Signing AWS API requests
- when we call the AWS HTTP API, we sign the request so that AWS can identify us, using our AWS credentials(access key & secret key)
-NOTE: some requests to Amazon S3 don't need to be signed
- if we use the SDK or CLI, the HTTP requests are signed for us
- we should sign an AWS HTTP request using Signature v4(SigV4)

==========================================================================================================================

Advanced S3


Amazon S3 

- Moving between storage Classes
-we can transistion objs between storage classes
-for infrequently accessed obj, move them to std-IA
-for archive objs that don't need fast access to, move them to Glacier or Glacier Deep Archive
-moving objs can be automated using Lifecycle Rules


Amazon S3- LifeCycle Rules

Transition Actions	: configure objs to transition to another storage class
- move objs to std-IA class 60 days after creations
- move to Glacier for archiving after 6 months
Expiration actions	: configure objs to expire(delete) after sometime 
- access log files can be set to delete after 365 days 
- can be used to delete old versions of files(if versioning is enabled)
- can be used to delete incomplete Multi-Part uploads
Rules can be created for a certain prefix(eg: s3://mybucket/mp3/*)
Rules can be created for a certain objs Tags( eg: Department: Finance)

scenario 1 (for Lifecycle rules)
Your application on ec2 creates imgs thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 60 days. The source imgs should be able to be immediatley retrieved for these 60 days, and afterwards, the user can wait up to 6 hrs. How would you design this?
- S3 source imgs can be on std, with a lifecycle configuratin to transition them to Glacier after 60 days
-S3 thumbnails can be on One-Zone IA, with a lifecycle configuration to expire them after 60 days.

scenario 2 (for lifecycle rule)
A rule in your company states that you should be able recover your deleted S3 objs immediately for 30 days, although this may happen rarely. After this time, and for up to 365 days, deleted objs should be recoverable within 48 hrs.
- Enable S3 versioning inorder to have obj versions, so that " deleted objs" are in fact hidden by a "delete marker" and can be recovered
- Transition the "noncurrent versions" of the obj to std-IA after 30days
- Transition afterwards the "noncurrent versions"  to Glacier Deep Archive after 365 days

Amazon S3 Analytics - Storage Class Analysis
-helps us to decide when to transition objs to the right storage class
- recommendations for std and std-IA (does not work for One-zone IA or Glacier)
-report is updated daily
-24 to 48 hrs to start seeing data analysis
- good first step to put together lifecycle rules

S2 Event Notifications
-s3: ObjectCreated, s3: ObjectRemoved, s3: ObjectRestore, s3: Replication...
- obj name filtering possible(*.jpg)
-usecase: generate thumbnails of imgs uploaded to s3 
- can create as many "s3 events" as desired 
- s3 event notifications typically deliver events in seconds but can sometimes take a min or longer

s3 event notifications - IAM permissions
s3 can have s3 events notifications by IAM permissions to SNS, SQS and lambda functions by using SNS resource (Access) policy, SQS resource(Access) policy and lambda Resouce policy.

s3 event notifications with Amazon EventBridge
events->s3 bucket->amazon eventbridge ->over 19=8 aws services as destinations by rules
- advanced filtering options with JSON rules(metadata, obj size,name)
- Multiple Destinations- ex step functins, kinesis streams/firehose...
- EventBridge Capablilities- Archive, Replay Events, Reliable delivery

S3 - Baseline Performance
-amazon s3 automatically scales to high request rates, latency 100-200 ms
- our application can achieve atleast 3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests per second per perfix in a bucket
- there are no limits to the number of prefixes in a bucket 
eg: (obj path=> prefix):
bucket/folder1/sub1/file=> /folder1/sub1/
bucket/folder1/sub2/file=> /folder1/sub2/
bucket/1/file => /1/
bucket/2/file => /2/
- if we spread reads across all four prefixes evenly, we can achieve 22000 requests per second for GET and HEAD

S3 Performance
Multi-Part upload:
-recommedaton for files> 100MB, must use for files >5GB
- can help parallelize uploads(speed up transfers)
eg: big file(divide in parts) ==(parallel uploads)==> s3 
S3 Transfer Acceleration:
- increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
- Compatible with multi-part upload
eg: file in USA ---(fast via public www)--> edge location --(fast via private AWS----> s3 bucket

S3 performance - s3 Byte-Range Fetches
-parallelize GETs by requesting specific byte ranges
- better resilience in case of failures
can be used to speed up downloads
-[figure]if we have a file in s3 we can achieve by request parts in parallel 
can be used to retrieve only partial data (for eg the head of a file)
-[fig] byte-range request for header(first xx bytes) from a file in s3

S3 Select & Glacier Select
-retrieve less dat using SQL by performing server-side filtering 
- can filter by rows & columns(simple SQL statements)
- less network transfer, less CPU cost client-side
by using this we can have 400% faster and 80% cheaper and 
works like it gets CSV with s3 select from client to s3 bucket and CSV file filtered by server-side filtering and send filtered dataset to client

S3 Object Tags & Metadata

S3 User-Defined Object Metadata:
-when uploading an obj, we can also assign metadata
- Name-value(key-value) pairs
- User-defined metadata names must begin with "x-amz-meta"
- Amazon S3 stores user-defined metadat keys in lowercase
- Metadata can be retrieved while retrieving the object

S3 Object Tags:
-key value pairs for objects in Amazon S3 
- useful for fine-graining permissions (only access secific objs with specific tags)
- useful for analytics purposes(using s3 analytics to grp by tags)

we cannot search the object metadata or object tags
instead we must use an external DB as a search index such as DynamoDB 
============================================================================================================================

Amazon S3 Security

Amazon S3 - Object Encryption
we can encrypt objs in S3 buckets in 4 methods
Sever-Side Encryption (SSE)
-Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) -Enable by Default, encrypts s3 objs using keys handled , managed and owned by AWS
- Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)- leverage AWS Key Management Service(AWS KMS) to manage encryption keys
- Server-Side Encryption with Customer-Provided Keys(SSE-C)- when we want to manage our own encryption keys
Client-Side Encryption

Amazon S3 Encryption- SSE-S3
-Encryption using key handled, manage and owned by AWS
-object is encrypted server-side
-Encryption type is AES-256
-Must set header "x-amz-server-side-encryption":"AES-256"
-enabled by default for new buckets & new objects

Amazon S3 Encryption - SSE-KMS
-encryption using keys handled and managed by AWS KMS (key management service)
-KMS advantages : usercontrol + audit key usage using cloudtrail
-Object is encrypted server side
-Must set header "x-arm-server-side-encryption":"aws:kms"
-Limitations: 
if we use SSE-KMS, we may be impacted by KMS limits
when we upload, it calls the GenerateDataKey KMS API
when we download, it calls the Decrypt KMS API
Count towards the KMS quota per second(5500,10000,30000 req/s based on region)

Amazon S3 Encryption- SSE-C
-server-side Encryption using keys fully managed by the customer outside of AWS
-amazon s3 does NOT store the encryption key we provide
- HTTPS must be used
- Encryption key must provided in HTTP headers, for every HTTP request made

Amazon s3 Encryption - Client-Side Encryption
-use client libraries such as Amazon S3 Client-Side Encryption Library
-Clients must encrypt data themselves before sending to Amazon S3
-clients must decrypt data themselvees when retrieving from Amazon S3
-Customer fully manages the keys and encryption cycle

Amazon S3- Encryption in transit (SSL/TLS)
encryption in flight is also called SSL/TLS
Amazon S3 exposes two endpoints:
	-HTTP Endpoint - non encrypted
	-HTTPS Endpoint - encrypted
HTTPS is recommended
HTTPS is manadatory for SSE-C
most clients would use the HTTPS endpoint by default
 
DSSE-KMS double encryption based on KMS

Amazon S3- Default Encryption vs Bucket Policies
- SSE-S3 Encryption is automatically applied to new objects stored in S3 bucket
- Optionally, we can "force encryption" using a bucket policy and refuse any API call to PUT an S3 obj without encryption headers(SSE-KMS or SSE-C)
- NOTE: Bucket Policies are evaluated before "Default Encryption"

CORS

-CORS- Cross-Origin Resource Sharing
-Origin= scheme(protocol) + host(domain) + port
eg: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
-Web Browser based mechanism to allow requests to other origins while visiting the main origin
- Same origin: http://example.com/app1 & http://example.com/app2
- Different origins: http://www.example.com & http://other.example.com
- The requests won't be fullfilled unless the other origin allows for the requests, using CORS Headers(eg: Access-Control-Allow-Origin)

Amazon S3- CORS
- If a client makes a cross-origin request on our s3 bucket, we need to enable the correct CORS header
- we can allow for a specific origin or for *( all origins)

Amazon S3 - MFA Delete
-MFA(multi factor authentication) force users to generate a code on a device(usually a mobile phone or hardware) before doing important operations on S3
- MFA will be required to : 
permanently delete an obj on bucket
suspend versioning on the bucket
-MFA won't be required to :
enable versioning 
list deleted versions
- to use MFA Delete, Versionin must be enabled on the bucket
-only the bucket owner(root account) can enable /disable MFA Delete

S3 Access Logs
-for audit purpose, we can log all access to S3 buckets
- any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
- that data can be analyzed using data analysis tools
- the target logging bucket must be in the same AWS region
WARNING:
-do not set your logging bucket to be the monitored bucket
- it will create a logging loop and your bucket will grow exponentially

Amazon S3 - Pre-Signed URLs
- generate pre-signed URLs using S3 console, AWS CLI or SDK
- URL Expiration:
s3 console: 1min to 720 min( 12 hrs)
AWS CLI: configure expiration with -expires-in parameter in seconds(default 3600 sec, max 604800 secs ~ 168 hrs)
- Users given a pre-signed URL inherit the permissions of the user that generated the URL the URL for GET/PUT
eg: 
1. allow only logged-in users to download a premium video from S3 bucket
2. allow only ever-changing list of users to download files by generating URLs dynamically
3. allow temporarily a user to upload a file to a precise location in S3 bucket

S3 - Access Points
Amazon S3 Access Points, a feature of S3, simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets.
- Access points simplify security management for s3 buckets
- Each Access point has: its own DNS name(Internet Origin or VPC Origin) and 
			 an access point policy(similar to bucket policy) - manage security at scale

S3 - Access Points- VPC Origin 
- we can define the access point to be accessible only from within the VPC
- we must create a VPC Endpoint to access the Access Point(Gateway or Interface Endpoint)
- The VPC Endpoint Policy must allow access to the target bucket and Access point

S3 Object Lambda
- use AWS lambda functions to change the obj before it is retrieved by the caller application
- only one S3 bucket is needed, on top of which we create S3 Access Point and S3 Object Lambda Access Point
- usecases: 
redacting personally identifiable info for analytics or non-production envs
converting across data formats, such as converting XML to JSON 
resizing and watermarking images on the fly using caller-specific details, such the user who requested the object.

============================================================================================================================

CLOUDFRONT

AWS CloudFront
-content Delivery Network(CDN)
-improves read performance, content is cached at the edge
-improves users experience
-216 points of presence globally(edge locations)
-DDoS protection(because worldwide), integration with Shield, AWS Web Application Firewall

CloudFront - Origins
-S3 bucket:
for distributing files and caching them at the edge
enhanced security with cloudfront Origin Access Control(OAC)
OAC is replacing Origin Access Identity(OAI)
CloudFront can be used as an ingress(to upload files to S3)

Custom Orign(HTTP):
Application Load Balancer
EC2 instance
S3 website(must first enable the bucket as a static S3 website)
Any HTTP backend we want

CloudFront vs S3 Cross Region Replication
CloudFront:
Global Edge Netword
Files are cached for a TTL (maybe a day)
Great for static content that must be available everywhere
S3 Cross Region Replication:
must be setup for each region we want to replicate 
files are updated in near real-time
Read only
great for dynamic content that needs to be available at low-latency in few regions

CloudFront Caching
- the cache lives at each cloudfront Edge location
- CloudFront identifies each obj in the cache using the Cache Key
- we can maximize the Cache Hit ratio to minimize req to the origin
- we can invalidate part of the cache using the CreatInvalidation API


CloudFront Cache Key
- a unique identifier fore every obj in the cache
- by default, consists of hostname+resource portion of the URL
- if we have an application that serves up content that varies based on user, device, language, location...
- we can add other elements (HTTP headers, cookies, query strings) to the Cache Key using CloudFront Cache Policies.

CloudFront Policies - Cache Policy
-Cache based on :
HTTP Headers: None- Whitelist
Cookies: None - Whitlelist- Include All-Except-All
Query Strings: None- Whitelist-Include All-Except-All
-Control the TTL(0 sec to 1 year), can be set by the origin using the Cache-Control header, Expires header...
-Create our own policy or use Predefined Managed Policies
-All HTTP headers, cookies and query strings that we include in the Cache Key are automatically included in origin requests

CloudFront Caching - Cache Policy HTTP headers
-None:
Don't include any headers in the Cache Key (except default)
Headers are not forwarded(except default)
Best caching performance
- Whitelist:
only specified headers included in the Cache Key
Specified headers are also forwarded to Origin 

CloudFront Cache - Cache Policy Query Strings
-None:
don't include any query strings in the Cache Key
Query strings are not forwarded
-Whitelist
only specified query strings included in the Cache Key
only specified query strings are forwarded 
-Include All-Except
include all query strings in the Cache Key except the specified list
all query strings are forwarded except the specified list
-All
include all query strings in the Cache Key
all query strings are forwarded 
worst caching performance

CloudFront Policies - Origin Request Policy
-specify values that we want to include in origin requests without including them in the Cache Key(no duplicated cached content)
-we can include :
HTTP headers: None-Whitelist-All viewer headers options
Cookies: None-Whitelist-All
Query Strings: None-Whitelist-All
- ability to add CloudFront HTTP headers and Custom Headers to an origin request that were not included in the viewer request 
-create our own policy or use Predefined Managed Policies

CloudFront - Cache Invalidations
- In case we update the back-end origin, CloudFront doesn't know about it and will only get the refreshed content after the TTL has expired
- However, we can force an entire or partial cache refresh(thus bypassing the TTL) by performing a CloudFront Invalidation
- we can invalidate all files(*) or a special path(images/*)

CloudFront - Cache Behaviors
-configure different settings for a given URL path pattern
-eg: one specific cache behavior to images/*.jpg files on our origin web server
-Route to different kind of origins/origin grps based on the content type or path pattern 
/images/*
/api/*
/*(default cache behavior)
-when adding additional Cache Behavior, the Default Cache Behavior is always the last to be processed and is always/*

CloudFront- Cache Behavior - sign in page 
[image]there will be signed cookies for some caches where we can define to login if we want to access the s3 bucket 

CloudFront - maximize cache hits by separating static and dynamic distributions
we can have Static request on CDN layer where there will be no headers/session caching rules required for maximizing cache hits to have static content on s3 bucket and 
Dynamic requests with Cache based on correct headers and cookie for a Dynamic content(REST,HTTP server) on ALB+EC2 

CloudFront -ALB or EC2 as an origin 
we can connect edge location by allowing public IP of Edge Locations to EC2 instances which must be public 
but 
while using ALB we can connect ALB which is public from Edge locations public  IPs from there we can have EC2 instances which are private to ALB

CloudFront Geo Restriction 
- we can restrict who can access our distribution 
Allowlist: Allow our users to access our content only if they're in one of the countries on a list of approved countries
Blocklist: Prevent our users frrom accessing our content if they're in one of the countries on a list of banned countries
- The "country" is determined using a 3rd party Geo-IP database
- usecase: Copyright Laws to control access to content  

CloudFront Signed URL/Signed Cookies
-if we want to distribute paid shared content to premium users over the world
- we can use CloudFront Signed URL/Cookie. we attach a policy with :
include URL expiration
includes IP ranges to access the data from 
trusted signers(which AWS accounts can create signed URLs)
- how long should the URL be valid for?
shared content (movie,music): make it short(a few mins)
private content(private to the user): we can make it last for years
- Signed URL= access to individual files(one signed URL per file)
- Signed Cookies= access to multiple files(one signed cookie for many files)

CloudFront Signed URL vs S3 Pre-Signed URL

CloudFront Signed URL:
allow access to a path, no matter the origin 
Account wide key-pair, only the root can manage it
can filter by IP,path, data, expiration
can leverage caching features

S3 Pre-Signed URL:
issue a request as the person who pre-signed the URL 
uses the IAM key of the signing IAM principal
limited lifetime

CloudFront - Pricing
- CloudFront Edge locations are all around world
- The cost of data out per edge location varies (india has highest and us, mexico& Canada has least)

CloudFront - Price Classes
- we can reduce the no. of edge locations for cost reduction
- Three price classes:
1. Price Class All: all regions - best performance
2. Price Class 200: most regions, but excludes the most expensive regions(like india)
3. Price Class 100: only the least expensive regions

CloudFront - Multiple Origin 
- to route to different kind of origins based on the content type 
- based on path pattern:
/images/*
/api/*
/*

CloudFront - Origin Groups
- to increase high-availability and do failover
- Origin Group: one primary and one secondary origin 
- if the primary origin fails, the second one is used

CloudFront - Field Level Encryption
- protect user senitive information through application stack
- adds an additional layer of security along with HTTPS
- sensitive information encrypted at the edge close to user
- uses asymmetric encryption
- usage :
specify set of fields in POST requests that we want to be encrypted(upto 10 fields)
specify the public key to encrypt them 

CloudFront - Real Time Logs
- Get real-time requests received by 	
- monitor, analyze, and take actions based on content delivery performance
- allows you to chose:
Sampling Rate: percentage of requests of requests for which we want to receive 
Specific fields and specific Cache Behaviors(path patterns)

================================================================================================================================

ECS, ECR & FARGATE - Docker in AWS

What is Docker?
- software development platform to deploy apps
- Apps are packaged in containers that can be run on any OS
- Apps run the same, regardless of where they're run
Any machine
No compatibility issues
Predictable behavior
Less work
Easier to maintain and deploy
Works with any language, any OS, any technology
- usecases: microservcies architecture, lift-and-shift apps from on-premises to the AWS cloud,...

Where are Docker images stored?
- Docker images are stored in Docker Repositories
- Docker Hub(https://hub.docker.com)
public repository
Find base images for many technologies or OS(eg: Ubuntu, MySQL,...)
- Amazon ECR (Amazon Elastic Container Registry)
Private Repository
Public repository( Amazon ECR Public Gallery https://gallery.ecr.aws)

Docker versus Virtual Machines
- Docker is "sort of" virtualization technology, but not exactly
- Resources are shared with the host=> many containers on one server
for normal VMs : infrastructure->Host OS-> Hypervisor->Guest OS(vm) with Apps, Guest OS with Apps, Guest OS with Apps.
for Docker : infrastructure->Host OS(ec2 instance)->Docker Daemon with many containers which are lightweight and share networking and data etc.

Docker Containers Management on AWS

1. Amazon Elastic Container Service(Amazon ECS): amazon's own container platform
2. Amazon Elastic Kubernetes Service(Amazon EKS): amazon's managed Kubernetes(open Source)
3. AWS Fargate: amazon's own Serverless container platform and works well with ECS and with EKS
4. Amazon ECR: store container images


Amazon ECS - EC2 Launch Type
- ECS = Elastic Container Service
- Launch Docker containers on AWS = Launch ECS tasks on ECS Clusters
- EC2 Launch Type: we must provision & maintain the infrastructure(the EC2 instances)
- Each EC2 instance must run the ECS Agent to register in the ECS Cluster
- AWS takes care of starting / stopping containers.

Amzon ECS - Fargate Launch Type
- launch Docker containers on AWS
- we don't provision the infrastructure(no ec2 instances to manage)
- it's all Serverless
- we just create task definitions
- AWS just runs ECS Tasks for us based on the CPU/RAM we need
- to scale, just increase the no.of task. Simple - no more EC2 instances

Amazon ECS- IAM Roles for ECS
- EC2 Instance Profile(EC2 Launch Type only):
used by the ECS Agent
makes API calls to ECS service
send container logs to CloudWatch Logs
Pull Docker image from ECR
Reference sensitive data in Secrets Manager or SSM Parameter Store

-ECS Task Role:
allows each task to have specific role
use different roles for the different ECS services we run
Task Role is defined in the task definition

Amazon ECS - Load Balancer Integrations
- Application Load Balancer supported and works for most usecases
- Network Load Balancer recommended only for high throughput / high performance usecases, or to pair it with AWS Private Link
- Classic Load Balancer supported but not recommended(no advanced features - no Fargate)

Amazon ECS - Data Volumes(EFS)
- mount EFS file systems onto ECS tasks
- works for both EC2 and Fargate launch types
- tasks running in any Az will share the same data in the EFS file system
- Fargate + EFS = Serverless
- usecases: persistent multi-Az shared storage for containers
- NOTE: Amazon S3 cannot be mounted as a file system

ECS Service Auto Scaling
- automatically increase/decrease the desired no.of ECS tasks
- Amazon ECS Auto Scaling uses AWS Application Auto Scaling
it's done on 	ECS Service Average CPU Utilization
		ECS Service Average Memory Utilization - Scale on RAM
		ALB Request Count Per Target - metric coming from the ALB
- Target Tracking: scale based on target value for a specific CloudWatch metric
- Step Scaling: scale based on a specific CloudWatch Alarm breaching
- Scheduled Scaling: scale based on specified date/time(predictable changes)
- ECS Service Auto Scaling(task level) is not equal to EC2 Auto Scaling(EC2 instance level)
- Fargate Auto Scaling is much easier to setup(because Serverless)

EC2 Launch Type - Auto Scaling EC2 Instances
- Accommodate ECS Service Scaling by adding underlying EC2 instances
- Auto Scaling Group Scaling:
	- scale ASG based on CPU Utilization
	- add EC2 instances over time
- ECS Cluster Capacity Provider:
	- used to automatically provision and scale the infrastructure for ECS tasks
	- Capacity Provider paired with an ASG
	- add EC2 Instances when there is missing capacity(CPU,RAM)

ECS Rolling Updates
- when updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order.
we have minimum healthy percent and maximum percent by these we will decrease the capcity to minimum capacity and add the updated versions as maximum percent and decrease to minimum by removing previous versions and change them to v2.

Amazon ECS Deep dive topics

Amazon ECS - Task Definitions
- Task definitions are metadata in JSON form to tell ECS how to run a Docker container
- it contains crucial information, such as:
Image name
Port Binding for Container and Host 
Memory and CPU required
Environmental variables
Networking information
IAM Role
Logging configuration(eg: CloudWatch)
- can define upto 10 containers in a Task Definition

Amazon ECS - Load Balancing(EC2 Launch Type)
- we get a Dynamic Host Port Mapping if we define only the container port in the task definition 
- the ALB finds the right port on our EC2 Instances
- we must allow on the EC2 instance's Security Group any port from the ALB's Secruity Group

Amazon ECS - Load Balancing (Fargate)
- each task has a unique private IP
- only define the container port(host port is not applicable)
eg: 
1. ECS ENI Security Group - allow port 80 from the ALB
2. ALB Security Group - allow port 80/443 from web

Amazon ECS One IAM Role per Task Definition
- only Task Definition can have the IAM Role if the services in them need to use ECS task Role then they have to inherit from the Task Definition.
- every 
- Role is defined at task definition level not on service level, therefore all tasks in our service will have access to the role actions

Amazon ECS - Environment Variables
Task Definition can have Environment Variables are come from 
- Hardcoded - eg: URLs
- SSM Parameter Store - sensitive variables (eg: API keys, shared configs)
- Secrets Manager - sensitive variables (eg: DB passwords)
Environment Files(bulk)  - Amazon S3

Amazon ECS - Data Volumes(Bind Mounts)
- share data between multiple containers in the same Task Definition
- works for both EC2 and Fargate tasks
- EC2 Tasks - using EC2 instance storage: Data are tied to the lifecycle of the EC2 instance
- Fargate Task - using ephemeral storage: Data are tied to the container(s) using them and 20-200 GiB(default 20 GiB)
- usecases: 
1. share ephemeral data between multiple containers
2. "Sidecar" container pattern, where the "sidecar" container used to send metrics/logs to other destinations

ECS Tasks Placements
- when a task of type EC2 is launched, ECS must determine where to place it, with the constraints of CPU, memory, and available port.
- similarly, when a service scales in, ECS needs to determine which task to terminate.
- to assist with this, we can define a task placement strategy and task placement constraints.
- NOTE: this is only for ECS with EC2, not for Fargate.

ECS Task Placement Process:
Task placements strategies are a best effort
- when Amazon ECS places tasks, it uses the following process to select container instances:
1. identify the instances that satisfy the CPU, memory, and port requirements in the task definition
2. identify the instances that satisfy the task placement constraints 
3. identify the instances that satisfy the task placement strategies
4. select the instances for task placement.

ECS Task Placement Strategies
1. Binpack:
place tasks based on the least available amount of CPU or memory
this minimizes the no.of instances in use(cost savings)
JSON: "placementStrategy":[
		{
			"field":"memory"
			"type":"binpack"
		}
	]

2. Random: place the task randomly
Json:
"placementStrategy": 	[
	{
		"type":"random"
	}
]

3. Spread:
place the task evenly based on the specified value
eg: instanceId, attributes:ecs.availability-zone"
JSON:
"placementStrategy":[
	{
		"field":"attribute:ecs.availability-zone",
		"type":"spread"
	}
]

- we can also mix them together
eg:1. JSON:
"placementStrategy":[
	{
		"field":"attribute:ecs.availability-zone",
		"type":"spread"	
	},
	{
		"field":"instanceId"
		"type":"spread"	
	}
]

2. JSON:
"placementStrategy":[
	{
		"field":"attribute:ecs.availability-zone",
		"type":"spread"	
	},
	{
		"field":"memory",
		"type":"binpack"	
	}
]

ECS Task Placement Constraints
- distinctInstance: place each task on a different container instance
"placementContraints":[
	{


		"type":"distinctInstance"	
	}
]
- memberOf: places task on instances that satisy an expression 
uses the Cluster Query Language
"placementConstraints":[
	{
		"expression":"attribute:ecs.instance-type=~ t2.*",
		"type":"memberOf"	
	}
]

Amazon ECR(Elastic Container Registry)
- store and manage Docker images on AWS
- Private and Public repository(Amazon ECR Public Gallery)
- Fully integrated with ECS, backed by Amazon S3
- Access is controlled through IAM(permission erros=> policy)
- supports image vulnerability scanning, versioning, image tags, image lifecyle...

Amazon ECR - Using AWS CLI

Login command- AWS CLI v2
aws ecr get-login-password --region | docker login --username AWS --password-stdin AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com

Docker Commands:
Push: docker push AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/demo:latest
Pull: docker pull AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/demo:latest
-in case an EC2 instance(or we) can't pull a Docker image, check IAM permissions

AWS Copilot
- CLI tool to build, release, and operate production-ready containerized apps
- run our apps on AppRunner, ECS, and Fargate
- helps us focus on building apps rather than setting up infrastructure
- provisions all required infrastructure for containerzed apps (ECS,VPC,ELB,ECR...)
- automated deployments with one command using CodePipeline
- deploy to multiple environments
- troubleshooting, logs, health status...

- use CLI or YAML to describe the architecture of our applications in the Microservice way. Then we will use Copilot CLI to containerize our applications and deploy them. then we get Well-architected infrastructure setup, Deployment Pipeline and Effective Operations and Troubleshooting

Amazon EKS overview(Elastic Kubernetes Service)
- It is a way to launch managed Kubernetes clusters on AWS
- Kubernetes is an open-source system for automatic deployment, scaling and management of containerized application
- it's an alternative to ECS, similar goal but different API
- EKS supports EC2 if we want to deploy worker nodes or Fargate to deploy serverless containers
- usecases:if a company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes
- Kubernetes is cloud-agnostic(can be used in any cloud- Azure, GCP,...)

Amazon EKS- Node Types
Managed Node Groups:
- creates and manages Nodes (ec2 instances) for us
- Nodes are part of an ASG managed by EKS
- Supports On-Demand or Spot Instances

Self-Managed Nodes
- Nodes created by us and registered to the EKS cluster and managed by an ASG
- we can use prebuilt AMI-Amazon EKS Optimized AMI
- supports On-Demand or Spot Instances

AWS Fargate
- No maintenance required, no nodes managed

Amazon EKS- Data Volumes
- need to specify StorageClass manifest on our EKS cluster
- Leverages a Container Storage Interface(CSI) compliant driver
- support for Amazon EBS, Amazon EFS(works with Fargate), Amazon FSx for Lustre, Amazon FSx for NetApp ONTAP

==============================================================================================================================

AWS Elastic Beanstalk 

Elastic Beanstalk- Overview
- Elastic Beanstalk is a developer centric view of deploying an application on AWS
- it uses all the component's we've seen before: EC2,ASG,ELB,RDS,...
- managed service:
automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration,...
just the application code is the responsibility of the developer
- we still have full control over the configuration
- Beanstalk is free but we pay for the underlaying instances

Elastic Beanstalk - Components
Application: collection of Elastic Beanstalk components(envs,versions,configs)
Application version: an iteration of our application code
Environment:
collection of AWS resources running an application version(only one application version at a time)
Tiers: Web Server Enviornment Tier & Worker Environment Tier
we can create multiple envs(dev,test,prod,...)

1. create applicaton 
2. upload version(if this is updated version we can directly deploy )
3. launch env
4. manage env(we can update version from here)  

Elastic Beanstalk - Supported Platforms
Go, Java SE, Java with Tomcat, .NET Core on Linux, .NET on Windows Server, Node.js, PHP, Python, Ruby, Packer Builder, Single Container Docker, Multi-container Docker, Preconfigured Docker
- if not supported, we can write our custom platform

Web Server Tier vs Worker Tier:
the difference is in Worker Env there is SQS Queue to send SQS messages to EC2 instances where as in Web Server Env there will ELB to instances 

in Worker env scale based on the no.of SQS messages and can push messages to SQS queue from another Web Server Tier

Elastic Beanstalk Deployment Modes:
1. Single Instance Great for dev 
2. High Availability with Load Balancer Great for prod 

Beanstalk Deployment Options for Updates
1. All at once(deploy all in one go):
- fastest deployment 
- applicaton has downtime
- great for quick iterations in development env
- no additional cost
but instances aren't available to serve traffic for a bit(downtime)

2. Rolling: update a few instances at a time(bucket), and then move onto the next bucket once the first bucket is healthy
- application is running below capacity
- can set the bucket size
- application is running both versions simultaneously
- no additional cost
- long deployment

3. Rolling with additional batches: like rolling, but spins up new instances to move the batch(so that the old application is still available)
- application is running at capacity
- can set the bucket size
- application is running both versions simultaneously
- small additional cost
- additional batch is removed at the end of the deployment
- longer deployment
- good for prod


4. Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy
- zero downtime
- New Code is deployed to new instances on a temporary ASG
- High Cost, double capacity
- longest deployment
- quick rollback in case of failures(just terminate new ASG)
- great for prod

5. Blue Green: create a new env and switch over when ready
-  not a "direct feature" of Elastic Beanstalk
- zero downtime and release facility
- create a new "stage" env and deploy v2 there
- the new env(green) can be validated independently and roll back if issues
- Route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage env
- using Beanstalk, "swap URLs" when done with the env test


6. Traffic Splitting: canary testing- send a small % of traffic to new deployment
- Canary Testing
- new application version is deployed to a temporary ASG with the same capacity
- a small % of traffic is sent to the temporary ASG for a configurable amount of time
- deployment health is monitored
- if there's a deployment failure, this triggers an automated rollback(very quick)
- no application downtime
- new instances are migrated from the temporary to the original ASG
- old application version is then terminated

Elastic Beanstalk CLI
-we can install an additional CLI called the "EB cli" which makes working with Beanstalk from the CLI easier
-Basic commands are:
eb create
eb status
eb health
eb events
eb logs
eb open
eb deploy
eb config
eb terminate
- it's helpful for our automated deployment pipelines

Elastic Beanstalk Deployment Process
- describe dependencies(requirements.txt for Python, package.json for Node.js)
- Package code as zip, and describe dependencies
Python : requirements.txt
Node.js: package.json
- Console: upload zip file (creates new app version), and then deploy
- CLI: create new app version using CLI(uploads zip), and the deploy
- Elastic Beanstalk will deploy the zip on each EC2, resolve dependencies and start the application

Beanstalk Lifecycle Policy
- Elastic Beanstalk can store at most 1000 application versions
- if we don't remove old versions, we won't be able to deploy anymore
- to phase out old application versions, use a lifecycle policy
	-based on time(old versions are removed)
	-based on space(when we have too many versions)
- versions that are currently used won't be deleted
- option not to delete the source bundle in S3 to prevent data loss

Elastic Beanstalk Extensions
- a zip file containing our code must be deployed to Elastic Beanstalk
- all the parameters set in the UI can be configured with code using files
- requirements:
	-in the .ebextensions/ directory in the root of source code
	-YAML / JSON format
	-.config extensions(eg: logging, config)
	-able to modify some default setting using: option_setting
	-ability to add resources such as RDS, ElastiCache, DynamoDB, etc..
- resources managed by .ebextensions get deleted if the environment goes away


Elastic Beanstalk Under the Hood
- under the hood, Elastic Beanstalk relies on CloudFormation
- CloudFormation is used to provision other AWS 
- usecase: we can define CloudFormation resources in our .ebextensions to provision ElastiCache, an S3 bucket, anything we want

Elastic Beanstalk Cloning
- clone an env with the exact same configuration
- useful for deploying a "test" version of our application
- all resources and configuration are preserved:
Load Balancer type and configuration
RDS database type( but the data is not preserved)
Environment variables
- after cloning an env, we can change setting

Elastic Beanstalk Migration: Load Balancer
- after creating an Elastic Beanstalk env, we cannot change the Elastic Load Balancer type (only the configuration)
- to migrate:
1. create a new env with the same configuration except LB(can't clone)
2. deploy our application onto the new env
3. perform a CNAME swap or Route 53 update

RDS with Elastic Beanstalk
- RDS can be provisioned with Beanstalk, which is great for dev/test
- this is not great for prod as the database lifecycle is tied to the Beanstalk env lifecycle
- the best for prod is to separately create an RDS database and provide our EB application with the connection string

Elastic Beanstalk Migration: Decouple RDS
1. create a snapshot of RDS DB(as a safeguard)
2. go to the RDS console and protect the RDS database from deletion
3. create a new Elastic Beanstalk env, without RDS, point application to existing RDS
4. perform a CNAME swap(blue/green) or Route 53 update, confirm working
5. terminate the old env (RDS won't be deleted)
6. delete CloudFormation stack (in DELETE_FAILED state)
============================================================================================================================================

AWS CloudFormation

Infrasturcture as Code
- all we did was a lot of manual work 
- all this manual work will be very tough to reproduce:
in another region
in another AWS account
within the same region if everything is deleted
- wouldn't be great if, all our infrastructure was... code?
- that code would be deployed and create/update/delete our infrastructure

What is CloudFormation
- CloudFormation is a declarative way of outlining our AWS infrastructure, for any resources
- for eg, within a CloudFormation template, we say:
i want a security group
i want two EC2 machines using this security group
i want two Elastic IPs for these EC2 machines
i want an S3 bucket
i want a load balancer (ELB) in front of these machines
- then CloudFormation creates those for us, in the right order, with the exact configuration that we specify

Benefits of AWS CloudFormation
1. Infrastructure as Code
- no resources are manually created, which is excellent for control
- the code can be version controlled for example using git
- changes to the infrastructure are reviewed through code
2. cost
- each resources within the stack is stagged with an identifier so we can easily see how much a stack costs us
- we can estimate the costs of our resources using the CloudFormation template
- savings strategy: in DEV, we could automation deletion of templates at 5 pm and recreatd at 8 am, safely
3. productivity
- ability to destroy and re-create an infrastructure on the cloud on the fly
- automated generation of Diagram for our templates.
- declarative programming(no need to figure out ordering and orchestration)
4. separation of concern: 
- create many stacks for many apps, and many layers eg: VPC stacks, Network stacks, App stacks
5. don't re-invent the wheel
- leverage existing templates on the web
- leverage the documentation

How CloudFormation Works
- Templates have to be uploaded in S3 and then referenced in CloudFormation
- To update a template, we can't edit previous ones. we have to re-upload a new version of the template to AWS
- stacks are identified by a name
- deleting a stack deletes every single artifact that was created by CloudFormation

Deploying CloudFormation templates
Manual way:
- editing templates in the CloudFormation Desinger
- using the console to input parameters, etc

Automated way:
- editing templates in a YAML file
- using the AWS CLI (Command Line Interface) to deploy the templates
- recommended way when we finally want to automate our flow 

CloudFormation Building Blocks
templates components :
1. Resources: our AWS resources declared in the templates(mandatory)
2. Parameters: the dynamic inuts for template
3. Mappings: the static variables for template
4. Outputs : references to what has been created
5. Conditionals: list of conditions to perform resource creation
6. Metadata

Template helpers:
1. References
2. Functions

YAML crash course
- YAML and JSON are the languages we can used for CloudFormation 
- JSON is horrible for CF
- YAML is great in so many ways like
Key value pairs
Nested objects
Support Arrays
Multli line strings
can include comments

CloudFormation Resources
- Resources are the core of our CloudFormation template(mandatory)
- they represent the different AWS components that will be created and configured 
- Resources are declared and can reference each other 
- AWS figures out creation, updated and deletes of resources for us 
- there are over 224 types of resources
- resource types identifiers are of the form:
AWS:: aws-product-name::data-type-name

- can we create a dynamic amount of resource?
no, we can't. everything in the CloudFormation template has to be declared. we can't peform code generation there 

- is every AWS service supported?
almost. only a select few niches are not there yet
we can work around that using AWS Lambda Custom Resources.

CloudFormation Parameters
- Parameters are a way to provide inputs to our AWS CloudFormation template
- they're important to know about if:
if we want to reuse our templates across the company
some inputs can not be determined ahead of time
- Parameters are extremely powerful, controlled, and can prevent errors from happening in our templates thanks to types.

when should we use a parameter?
ask questions like :
is this CloudFormation resource configuration likely to change in the future?
if so, make it a parameter

- we won't have to re-upload a template to change its content

Parameters Settings:
parameters can be controlled by all these settings:
1. Type:
String, Number, CommaDelimitedList, List<Type>, AWS Parameter
2. Description
3. Constraints
4. ConstraintDescription(string)
5. Min/MaxLength
6. Min/MaxValue
7. Defaults
8. AllowedValues (array)
9. AllowedPattern (regexp)
10. NoEcho (Boolean)

how to Reference a Parmeter 
- the Fn::Ref function can be leveraged to reference parameters
- Parameters can be used anywhere in a template
- the shorthand for this in YAML is !Ref

Pseudo Parameters
- AWS offers us pseudo parameters in any CloudFormation template
- these can be used at any time and are enabled by default

CloudFormation Mappings:

- Mappings are fixed variables within our CloudFormation Template
- they're very handy to differentiate between different envs(dev vs prod), regions( AWS regions), AMI types, etc
- all the values are hardcoded within the template

when would we use mappings vs parameters?
- Mappings are great when we know in advance all the values that can be taken and that they can be deduced from the variables such as 
Region
Availability Zone
AWS Account
Environment (dev or prod) etc
- they allow safer control over the template 
- use parameters when the values are really user specific 

Fn::FindInMap Accessing Mapping Values:
- we use Fn::FindInMap to return a named value from a specific key
- !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]

CloudFormation Outputs:

- the Outputs section declares optional outputs values that we can import into other stacks(if we export them first)
- we can also view the outputs in the AWS Console or in using the AWS CLI
- they're very useful for example if we define a network CloudFormation, and output the variables such as VPC and our Subnet IDs
- it's the best way to perform some collboration cross stack, as we let expert handle their own part of the stack
- we can't delete a CloudFormation Stack if its outputs are being referenced by another CloudFormation stack

eg: 
creating a SSH Security Group as part of one template
we create an output that references that security group 
=>check slide 408 image

Cross Stack Reference
- we then create a second template that leverage that security group
- for this, we use the Fn:: ImportValue function
- we can't delete the underlying stack until all the references are deleted too.
=> check slide 409

CloudFormation Conditions

- conditions are used to control the creation of resources or outputs based on a condition
- conditions can be whatever we want them to be, but common ones are 
environment (dev/test/prod)
AWS region
any parameter value
- each condition can reference another, parameter value or mapping
 
how to define a condition?
eg:
Conditions:
	CreateProdResources: !Equals [ !Ref EnvType, prod ]

- the logical ID is for us to choose. it's how we name condition
- the intrinsic function(logical) can be any of the following 
Fn:: And
Fn:: Equals
Fn:: If
Fn:: Not
Fn:: Or

- conditions can be applied to resources/outputs/etc..
eg:
Resources:
  MountPoint:
    Type: "AWS:EC2::VolumeAttachement"
    Condition: CreateProdResources

CloudFormation Intrinsic Functions

- Ref
- Fn::GetAtt
- Fn::FindInMap
- Fn::ImportValue
- Fn::Join
- Fn::Sub
- Condition Functions( Fn::If, Fn::Not, Fn::Equals, etc...)

Fn::Ref
- the Fn::Ref function can be leveraged to reference
Parameters=> returns the value of the parameter
Resources => returns the physical ID of the underlying resource (eg: EC2)
- the shorthand for this is YAML is !Ref
eg:
DbSubnet1:
  Type: AWS::EC2::Subnet
  Properties::
    VpcId: !Ref MyVPC

Fn::GetAtt
- attributes are attached to any resources we create
- to know the attributes of our resources, the best place to look at is the documentation 
- for eg: the Az of an EC2 machine

eg:
Resources:
  EC2Instances:
    Type: "AWS::EC2::Instance"
    Properties:
      ImageId: ami-1234567
      InstanceType: t2.micro

NewVolume:
  Type: "AWS:EC2::Volume"
  Condition: CreateProdResources
  Propertiees:
    Size: 100
    AvailabilityZone:
      !GetAtt: EC2Instance.AvailabilityZone

Fn::FindInMap Accessing Mapping Values
- we use Fn::FindInMap to return a named value from a specific key
- !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]   

Fn::ImportValue
- import values that are exported in other templates
- for this, we use the Fn::ImportValue function

Fn::Join
- Join values with a delimiter
syntax: !Join [ delimiter, [ comma-delimited list of values ] ] 
eg:  !Join [ ":", [a,b,c]] this creates "a:b:c"

Fn:: Sub
- Fn::Sub or !Sub as a shorthand, is used to substitute variables from a text. it's a very handy function that will allow us to fully customize your templates.
- for eg: we can combine Fn::Sub with Reference or AWS Pseudo variables.
- String must contain ${VariableName} and will substitute them 
!Sub 
  - String
  - { Var1Name: Var1Value, Var2Name : Var2Value }

Condition Functions
Conditions: 
  CreateProdResources: !Equals [ !Ref EnvType, prod ]
- the logical ID is for us to choose. It's how we name condition 
- the intrinsic functin(logical) can be any of the following:
Fn::And
Fn::Equals
Fn::If
Fn::Not
Fn::Or


CoudFormation Rollbacks

Stack Creation Fails:
- Default: everything rolls back (gets deleted). we can look at the log
- option to disable rollback and troubleshoot what happened

Stack Update Fails:
- The stack automatically rolls back to the previous known working state
- ability to see in the log what happened and error messages

CloudFormation Stack Notifications
- send Stack events to SNS Topic (Email,Lambda,..)
- enable SNS Integration using Stack Options

ChangeSets
- When we update a stack, we need to know what changes before it happens for greater confidence
- ChangeSets won't say if the update will be successful 

Nested stacks
- Nested stacks are stacks as part of other stacks
- they allow us to isolate repeated patterns/common components in separate stacks and call them from other stacks
- eg: load balancer configuration that is re-used , security group that is re-used
- Nested stacks are considered best practice
- to update a nested stack, always update the parent(root stack)

CloudFormation - Cross vs Nested Stacks
Cross Stacks
- helpful when stacks have different lifecycles
- use Outputs Export and Fn::ImportValue
- when we need to pass export values to many stacks(VPC Id, etc...)

Nested Stacks
- helpful when components must be re-used 
- eg: re-use how to properly configure an Application Load Balancer
- the nested stack only is important to the higher level stack(it's not shared)

CloudFormation - StackSets
- create, update, or delete stacks across multiple accounts and regions with a single operation.
- Administrator account to create StackSets
- Trusted accounts to created, update, delete stack instances from StackSets
- when we update a stack set, all associated stack instances are updated throughout all accounts and regions.

CloudFormation Drift
- CloudFormation allows us to create infrastructure
- but it doesn't protect us against manual configuration changes
- we can use CloudFormation drift to know if our resources have drifted

CloudFormation Stack Policies
- during a CloudFormation Stack update, all update actions are allowed on all resources(default)
- A stack Policy is a JSON document that defines the update actions that are allowed on specific resources during Stack updates
- Protect resources from unintentional updates
- when we set a Stack Policy, all resources in the Stack are protected by default
- specify an explicit ALLOW for the resources we want to be allowed to be updated.
- allow updates on all resources except the ProductionDatabase


